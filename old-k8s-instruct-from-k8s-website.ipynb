{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq_jU3DRdmVj"
   },
   "source": [
    "# Generate a K8s dataset for LLMs\n",
    "\n",
    "This notebook reverse engineers a possible prompt for K8s YAML manifests from the K8s website. It does this by calling ChatGPT API with following prompt:\n",
    "```\n",
    "f\"Explain in 3 sentences and start your explanation with 'Write YAML that':\\n```yaml\\n{YAM_FILE}\\n```\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvfHqLCke0GK",
    "outputId": "9612d17d-c8b9-408d-d888-b383a4f765b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'website'...\n",
      "remote: Enumerating objects: 352325, done.\u001b[K\n",
      "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
      "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
      "remote: Total 352325 (delta 82), reused 114 (delta 54), pack-reused 352164\u001b[K\n",
      "Receiving objects: 100% (352325/352325), 399.73 MiB | 21.71 MiB/s, done.\n",
      "Resolving deltas: 100% (256710/256710), done.\n",
      "Updating files: 100% (8097/8097), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/kubernetes/website.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDWtdko6fQWB",
    "outputId": "2509cf4a-b15a-4f68-d331-2f1cb8443a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access\t     concepts\t  examples.go\t    priority-and-fairness  service\n",
      "admin\t     configmap\t  examples_test.go  README.md\t\t   tls\n",
      "application  controllers  pods\t\t    secret\t\t   windows\n",
      "audit\t     debug\t  policy\t    security\n"
     ]
    }
   ],
   "source": [
    "! ls website/content/en/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgUoSLVRfwUN",
    "outputId": "e4860c7b-431c-437b-de71-29e44d63c751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "website/content/en/examples/windows/deploy-hyperv.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "path = \"website/content/en/examples\"\n",
    "\n",
    "example_manifests = []\n",
    "for path in Path(path).rglob('*.yaml'):\n",
    "  example_manifests.append(path)\n",
    "\n",
    "print(example_manifests[0])\n",
    "len(example_manifests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syfWpBY7hBLd",
    "outputId": "2079f97f-fad5-436f-d183-c979cd86beac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting openai\n",
      "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
      "Collecting aiohttp (from openai)\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.8 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByBFx62QhYCo",
    "outputId": "4f279509-ba07-4a9b-8846-b28b23ebe0ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ··········\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from getpass import getpass\n",
    "openai.api_key = getpass('OpenAI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TPZwISijhsMM",
    "outputId": "fcd9c89d-24cd-4bf4-9960-83d963ca9157"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'whisper-1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = openai.Model.list()\n",
    "models.data[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-WWPT6hiCfU",
    "outputId": "7cc15fa6-169a-4a0f-a208-eb549ca242d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo-0613\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
    "\n",
    "# print the chat completion\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8GC1sCkRjrAd",
    "outputId": "c5884742-9edb-4d19-eb47-251c5190e303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: iis\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: iis\n",
      "  replicas: 3\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: iis\n",
      "      annotations:\n",
      "        experimental.windows.kubernetes.io/isolation-type: hyperv\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: iis\n",
      "        image: microsoft/iis\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in example_manifests:\n",
    "  with path.open(mode=\"r\", encoding=\"utf-8\") as manifest:\n",
    "    content = manifest.read()\n",
    "    break\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "FbYilt7DSJst",
    "outputId": "b7488496-f2fe-40db-f19d-b0ab64bc3a1d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Write YAML that specifies a Kubernetes Deployment resource named \"iis\" in the apps/v1 API version. The deployment should create 3 replicas of a container based on the microsoft/iis image. The container should expose port 80 and have the label \"app: iis\" attached to it. Additionally, an annotation specifying hyperv as the isolation type for experimental Windows containers should be added to the Deployment\\'s template metadata.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_instruction(content: str) -> str:\n",
    "  chat_completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo-0613\",\n",
    "      messages=[{\n",
    "          \"role\": \"user\",\n",
    "          \"content\": f\"Explain in 3 sentences and start your explanation with 'Write YAML that':\\n```yaml\\n{content}\\n```\"\n",
    "          }])\n",
    "\n",
    "  return chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# print the chat completion\n",
    "get_instruction(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eejnnONAT2d8",
    "outputId": "060932f1-d7d5-4db2-bd87-3866ff69a808"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'Write YAML that defines a Deployment resource with the API version of apps/v1 and specifies that it creates instances of the kind Deployment. The metadata section defines the name of the deployment as \"iis\". The spec section sets the selector to match labels with the value of \"app: iis\" and specifies 3 replicas. Inside the template section, the metadata section adds labels and annotations, including specifying the isolation type as hyperv for Windows environments. Finally, the spec section inside the template defines a container named \"iis\" with the image of microsoft/iis and exposes port 80.',\n",
       "  'completion': '```yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: iis\\nspec:\\n  selector:\\n    matchLabels:\\n      app: iis\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: iis\\n      annotations:\\n        experimental.windows.kubernetes.io/isolation-type: hyperv\\n    spec:\\n      containers:\\n      - name: iis\\n        image: microsoft/iis\\n        ports:\\n        - containerPort: 80\\n\\n```'},\n",
       " {'prompt': 'Write YAML that defines a Kubernetes Pod object named \"iis\" with a label also named \"iis\". The Pod runs a container named \"iis\", with the image being microsoft/iis:windowsservercore-1709 and the container listening on port 80. Additionally, the Pod is scheduled on a node with the nodeSelector key \"kubernetes.io/os\" set to \"windows\".',\n",
       "  'completion': '```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: iis\\n  labels:\\n    name: iis\\nspec:\\n  containers:\\n    - name: iis\\n      image: microsoft/iis:windowsservercore-1709\\n      ports:\\n        - containerPort: 80\\n  nodeSelector:\\n    \"kubernetes.io/os\": windows\\n```'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k8s_dataset = []\n",
    "for path in example_manifests[0:2]:\n",
    "  with path.open(mode=\"r\", encoding=\"utf-8\") as manifest:\n",
    "    content = manifest.read()\n",
    "    k8s_dataset.append({\"prompt\": get_instruction(content), \"completion\": f\"```yaml\\n{content}\\n```\"})\n",
    "\n",
    "k8s_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94zsNsQrVmMy",
    "outputId": "eec085a1-14b2-4868-bfd2-88c97a4cf7a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```yaml\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: iis\n",
      "spec:\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: iis\n",
      "  replicas: 3\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: iis\n",
      "      annotations:\n",
      "        experimental.windows.kubernetes.io/isolation-type: hyperv\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: iis\n",
      "        image: microsoft/iis\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(k8s_dataset[0][\"completion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o30W6TwiWxG8",
    "outputId": "75e56763-4ae7-4c8c-d541-69031af552e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prompt\": \"Write YAML that defines a Deployment resource with the API version of apps/v1 and specifies that it creates instances of the kind Deployment. The metadata section defines the name of the deployment as \\\"iis\\\". The spec section sets the selector to match labels with the value of \\\"app: iis\\\" and specifies 3 replicas. Inside the template section, the metadata section adds labels and annotations, including specifying the isolation type as hyperv for Windows environments. Finally, the spec section inside the template defines a container named \\\"iis\\\" with the image of microsoft/iis and exposes port 80.\", \"completion\": \"```yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: iis\\nspec:\\n  selector:\\n    matchLabels:\\n      app: iis\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: iis\\n      annotations:\\n        experimental.windows.kubernetes.io/isolation-type: hyperv\\n    spec:\\n      containers:\\n      - name: iis\\n        image: microsoft/iis\\n        ports:\\n        - containerPort: 80\\n\\n```\"}\n",
      "{\"prompt\": \"Write YAML that defines a Kubernetes Pod object named \\\"iis\\\" with a label also named \\\"iis\\\". The Pod runs a container named \\\"iis\\\", with the image being microsoft/iis:windowsservercore-1709 and the container listening on port 80. Additionally, the Pod is scheduled on a node with the nodeSelector key \\\"kubernetes.io/os\\\" set to \\\"windows\\\".\", \"completion\": \"```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: iis\\n  labels:\\n    name: iis\\nspec:\\n  containers:\\n    - name: iis\\n      image: microsoft/iis:windowsservercore-1709\\n      ports:\\n        - containerPort: 80\\n  nodeSelector:\\n    \\\"kubernetes.io/os\\\": windows\\n```\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('k8s-instructions.jsonl', 'w') as outfile:\n",
    "    for entry in k8s_dataset:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "! cat k8s-instructions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmyTUXDoXY2k"
   },
   "outputs": [],
   "source": [
    "! rm k8s-instructions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Gm2OUTeIXczc",
    "outputId": "800ee3cc-60e5-4939-f8db-e58bffe1e4ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception occured while calling OpenAI. Skipping content: apiVersion: v1\n",
      "kind: ConfigMap\n",
      "data:\n",
      "  containers.input.conf: |-\n",
      "    # This configuration file for Fluentd is used\n",
      "    # to watch changes to Docker log files that live in the\n",
      "    # directory /var/lib/docker/containers/ and are symbolically\n",
      "    # linked to from the /var/log/containers directory using names that capture the\n",
      "    # pod name and container name. These logs are then submitted to\n",
      "    # Google Cloud Logging which assumes the installation of the cloud-logging plug-in.\n",
      "    #\n",
      "    # Example\n",
      "    # =======\n",
      "    # A line in the Docker log file might look like this JSON:\n",
      "    #\n",
      "    # {\"log\":\"2014/09/25 21:15:03 Got request with path wombat\\\\n\",\n",
      "    #  \"stream\":\"stderr\",\n",
      "    #   \"time\":\"2014-09-25T21:15:03.499185026Z\"}\n",
      "    #\n",
      "    # The record reformer is used to write the tag to focus on the pod name\n",
      "    # and the Kubernetes container name. For example a Docker container's logs\n",
      "    # might be in the directory:\n",
      "    #  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b\n",
      "    # and in the file:\n",
      "    #  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\n",
      "    # where 997599971ee6... is the Docker ID of the running container.\n",
      "    # The Kubernetes kubelet makes a symbolic link to this file on the host machine\n",
      "    # in the /var/log/containers directory which includes the pod name and the Kubernetes\n",
      "    # container name:\n",
      "    #    synthetic-logger-0.25lps-pod_default-synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n",
      "    #    ->\n",
      "    #    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\n",
      "    # The /var/log directory on the host is mapped to the /var/log directory in the container\n",
      "    # running this instance of Fluentd and we end up collecting the file:\n",
      "    #   /var/log/containers/synthetic-logger-0.25lps-pod_default-synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n",
      "    # This results in the tag:\n",
      "    #  var.log.containers.synthetic-logger-0.25lps-pod_default-synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n",
      "    # The record reformer is used is discard the var.log.containers prefix and\n",
      "    # the Docker container ID suffix and \"kubernetes.\" is pre-pended giving the tag:\n",
      "    #   kubernetes.synthetic-logger-0.25lps-pod_default-synth-lgr\n",
      "    # Tag is then parsed by google_cloud plugin and translated to the metadata,\n",
      "    # visible in the log viewer\n",
      "\n",
      "    # Example:\n",
      "    # {\"log\":\"[info:2016-02-16T16:04:05.930-08:00] Some log text here\\n\",\"stream\":\"stdout\",\"time\":\"2016-02-17T00:04:05.931087621Z\"}\n",
      "    <source>\n",
      "      type tail\n",
      "      format json\n",
      "      time_key time\n",
      "      path /var/log/containers/*.log\n",
      "      pos_file /var/log/gcp-containers.log.pos\n",
      "      time_format %Y-%m-%dT%H:%M:%S.%N%Z\n",
      "      tag reform.*\n",
      "      read_from_head true\n",
      "    </source>\n",
      "\n",
      "    <filter reform.**>\n",
      "      type parser\n",
      "      format /^(?<severity>\\w)(?<time>\\d{4} [^\\s]*)\\s+(?<pid>\\d+)\\s+(?<source>[^ \\]]+)\\] (?<log>.*)/\n",
      "      reserve_data true\n",
      "      suppress_parse_error_log true\n",
      "      key_name log\n",
      "    </filter>\n",
      "\n",
      "    <match reform.**>\n",
      "      type record_reformer\n",
      "      enable_ruby true\n",
      "      tag raw.kubernetes.${tag_suffix[4].split('-')[0..-2].join('-')}\n",
      "    </match>\n",
      "\n",
      "    # Detect exceptions in the log output and forward them as one log entry.\n",
      "    <match raw.kubernetes.**>\n",
      "      @type copy\n",
      "\n",
      "      <store>\n",
      "        @type prometheus\n",
      "\n",
      "        <metric>\n",
      "          type counter\n",
      "          name logging_line_count\n",
      "          desc Total number of lines generated by application containers\n",
      "          <labels>\n",
      "            tag ${tag}\n",
      "          </labels>\n",
      "        </metric>\n",
      "      </store>\n",
      "      <store>\n",
      "        @type detect_exceptions\n",
      "\n",
      "        remove_tag_prefix raw\n",
      "        message log\n",
      "        stream stream\n",
      "        multiline_flush_interval 5\n",
      "        max_bytes 500000\n",
      "        max_lines 1000\n",
      "      </store>\n",
      "    </match>\n",
      "  system.input.conf: |-\n",
      "    # Example:\n",
      "    # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script\n",
      "    <source>\n",
      "      type tail\n",
      "      format syslog\n",
      "      path /var/log/startupscript.log\n",
      "      pos_file /var/log/gcp-startupscript.log.pos\n",
      "      tag startupscript\n",
      "    </source>\n",
      "\n",
      "    # Examples:\n",
      "    # time=\"2016-02-04T06:51:03.053580605Z\" level=info msg=\"GET /containers/json\"\n",
      "    # time=\"2016-02-04T07:53:57.505612354Z\" level=error msg=\"HTTP Error\" err=\"No such image: -f\" statusCode=404\n",
      "    <source>\n",
      "      type tail\n",
      "      format /^time=\"(?<time>[^)]*)\" level=(?<severity>[^ ]*) msg=\"(?<message>[^\"]*)\"( err=\"(?<error>[^\"]*)\")?( statusCode=($<status_code>\\d+))?/\n",
      "      path /var/log/docker.log\n",
      "      pos_file /var/log/gcp-docker.log.pos\n",
      "      tag docker\n",
      "    </source>\n",
      "\n",
      "    # Example:\n",
      "    # 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal\n",
      "    <source>\n",
      "      type tail\n",
      "      # Not parsing this, because it doesn't have anything particularly useful to\n",
      "      # parse out of it (like severities).\n",
      "      format none\n",
      "      path /var/log/etcd.log\n",
      "      pos_file /var/log/gcp-etcd.log.pos\n",
      "      tag etcd\n",
      "    </source>\n",
      "\n",
      "    # Multi-line parsing is required for all the kube logs because very large log\n",
      "    # statements, such as those that include entire object bodies, get split into\n",
      "    # multiple lines by glog.\n",
      "\n",
      "    # Example:\n",
      "    # I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]\n",
      "    <source>\n",
      "      type tail\n",
      "      format multiline\n",
      "      multiline_flush_interval 5s\n",
      "      format_firstline /^\\w\\d{4}/\n",
      "      format1 /^(?<severity>\\w)(?<time>\\d{4} [^\\s]*)\\s+(?<pid>\\d+)\\s+(?<source>[^ \\]]+)\\] (?<message>.*)/\n",
      "      time_format %m%d %H:%M:%S.%N\n",
      "      path /var/log/kubelet.log\n",
      "      pos_file /var/log/gcp-kubelet.log.pos\n",
      "      tag kubelet\n",
      "    </source>\n",
      "\n",
      "    # Example:\n",
      "    # I1118 21:26:53.975789       6 proxier.go:1096] Port \"nodePort for kube-system/default-http-backend:http\" (:31429/tcp) was open before and is still needed\n",
      "    <source>\n",
      "      type tail\n",
      "      format multiline\n",
      "      multiline_flush_interval 5s\n",
      "      format_firstline /^\\w\\d{4}/\n",
      "      format1 /^(?<severity>\\w)(?<time>\\d{4} [^\\s]*)\\s+(?<pid>\\d+)\\s+(?<source>[^ \\]]+)\\] (?<message>.*)/\n",
      "      time_format %m%d %H:%M:%S.%N\n",
      "      path /var/log/kube-proxy.log\n",
      "      pos_file /var/log/gcp-kube-proxy.log.pos\n",
      "      tag kube-proxy\n",
      "    </source>\n",
      "\n",
      "    # Example:\n",
      "    # I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]\n",
      "    <source>\n",
      "      type tail\n",
      "      format multiline\n",
      "      multiline_flush_interval 5s\n",
      "      format_firstline /^\\w\\d{4}/\n",
      "      format1 /^(?<severity>\\w)(?<time>\\d{4} [^\\s]*)\\s+(?<pid>\\d+)\\s+(?<source>[^ \\]]+)\\] (?<message>.*)/\n",
      "      time_format %m%d %H:%M:%S.%N\n",
      "      path /var/log/kube-apiserver.log\n",
      "      pos_file /var/log/gcp-kube-apiserver.log.pos\n",
      "      tag kube-apiserver\n",
      "    </source>\n",
      "\n",
      "    # Example:\n",
      "    # 2017-02-09T00:15:57.992775796Z AUDIT: id=\"90c73c7c-97d6-4b65-9461-f94606ff825f\" ip=\"104.132.1.72\" method=\"GET\" user=\"kubecfg\" as=\"<self>\" asgroups=\"<lookup>\" namespace=\"default\" uri=\"/api/v1/namespaces/default/pods\"\n",
      "    # 2017-02-09T00:15:57.993528822Z AUDIT: id=\"90c73c7c-97d6-4b65-9461-f94606ff825f\" response=\"200\"\n",
      "    <source>\n",
      "      type tail\n",
      "      format multiline\n",
      "      multiline_flush_interval 5s\n",
      "      format_firstline /^\\S+\\s+AUDIT:/\n",
      "      # Fields must be explicitly captured by name to be parsed into the record.\n",
      "      # Fields may not always be present, and order may change, so this just looks\n",
      "      # for a list of key=\"\\\"quoted\\\" value\" pairs separated by spaces.\n",
      "      # Unknown fields are ignored.\n",
      "      # Note: We can't separate query/response lines as format1/format2 because\n",
      "      #       they don't always come one after the other for a given query.\n",
      "      # TODO: Maybe add a JSON output mode to audit log so we can get rid of this?\n",
      "      format1 /^(?<time>\\S+) AUDIT:(?: (?:id=\"(?<id>(?:[^\"\\\\]|\\\\.)*)\"|ip=\"(?<ip>(?:[^\"\\\\]|\\\\.)*)\"|method=\"(?<method>(?:[^\"\\\\]|\\\\.)*)\"|user=\"(?<user>(?:[^\"\\\\]|\\\\.)*)\"|groups=\"(?<groups>(?:[^\"\\\\]|\\\\.)*)\"|as=\"(?<as>(?:[^\"\\\\]|\\\\.)*)\"|asgroups=\"(?<asgroups>(?:[^\"\\\\]|\\\\.)*)\"|namespace=\"(?<namespace>(?:[^\"\\\\]|\\\\.)*)\"|uri=\"(?<uri>(?:[^\"\\\\]|\\\\.)*)\"|response=\"(?<response>(?:[^\"\\\\]|\\\\.)*)\"|\\w+=\"(?:[^\"\\\\]|\\\\.)*\"))*/\n",
      "      time_format %FT%T.%L%Z\n",
      "      path /var/log/kube-apiserver-audit.log\n",
      "      pos_file /var/log/gcp-kube-apiserver-audit.log.pos\n",
      "      tag kube-apiserver-audit\n",
      "    </source>\n",
      "\n",
      "    # Example:\n",
      "    # I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kubernetes-dashboard\n",
      "    <source>\n",
      "      type tail\n",
      "      format multiline\n",
      "      multiline_flush_interval 5s\n",
      "      format_firstline /^\\w\\d{4}/\n",
      "      format1 /^(?<severity>\\w)(?<time>\\d{4} [^\\s]*)\\s+(?<pid>\\d+)\\s+(?<source>[^ \\]]+)\\] (?<message>.*)/\n",
      "      time_format %m%d %H:%M:%S.%N\n",
      "      path /var/log/kube-controller-manager.log\n",
      "      pos_file /var/log/gcp-kube-controller-manager.log.pos\n",
      "      tag kube-controller-manager\n",
      "    </source>\n",
      "\n",
      "    # Example:\n",
      "    # W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]\n",
      "    <source>\n",
      "      type tail\n",
      "      format multiline\n",
      "      multiline_flush_interval 5s\n",
      "      format_firstline /^\\w\\d{4}/\n",
      "      format1 /^(?<severity>\\w)(?<time>\\d{4} [^\\s]*)\\s+(?<pid>\\d+)\\s+(?<source>[^ \\]]+)\\] (?<message>.*)/\n",
      "      time_format %m%d %H:%M:%S.%N\n",
      "      path /var/log/kube-scheduler.log\n",
      "      pos_file /var/log/gcp-kube-scheduler.log.pos\n",
      "      tag kube-scheduler\n",
      "    </source>\n",
      "\n",
      "    # Example:\n",
      "    # I1104 10:36:20.242766       5 rescheduler.go:73] Running Rescheduler\n",
      "    <source>\n",
      "      type tail\n",
      "      format multiline\n",
      "      multiline_flush_interval 5s\n",
      "      format_firstline /^\\w\\d{4}/\n",
      "      format1 /^(?<severity>\\w)(?<time>\\d{4} [^\\s]*)\\s+(?<pid>\\d+)\\s+(?<source>[^ \\]]+)\\] (?<message>.*)/\n",
      "      time_format %m%d %H:%M:%S.%N\n",
      "      path /var/log/rescheduler.log\n",
      "      pos_file /var/log/gcp-rescheduler.log.pos\n",
      "      tag rescheduler\n",
      "    </source>\n",
      "\n",
      "    # Example:\n",
      "    # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf\n",
      "    <source>\n",
      "      type tail\n",
      "      format multiline\n",
      "      multiline_flush_interval 5s\n",
      "      format_firstline /^\\w\\d{4}/\n",
      "      format1 /^(?<severity>\\w)(?<time>\\d{4} [^\\s]*)\\s+(?<pid>\\d+)\\s+(?<source>[^ \\]]+)\\] (?<message>.*)/\n",
      "      time_format %m%d %H:%M:%S.%N\n",
      "      path /var/log/glbc.log\n",
      "      pos_file /var/log/gcp-glbc.log.pos\n",
      "      tag glbc\n",
      "    </source>\n",
      "\n",
      "    # Example:\n",
      "    # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf\n",
      "    <source>\n",
      "      type tail\n",
      "      format multiline\n",
      "      multiline_flush_interval 5s\n",
      "      format_firstline /^\\w\\d{4}/\n",
      "      format1 /^(?<severity>\\w)(?<time>\\d{4} [^\\s]*)\\s+(?<pid>\\d+)\\s+(?<source>[^ \\]]+)\\] (?<message>.*)/\n",
      "      time_format %m%d %H:%M:%S.%N\n",
      "      path /var/log/cluster-autoscaler.log\n",
      "      pos_file /var/log/gcp-cluster-autoscaler.log.pos\n",
      "      tag cluster-autoscaler\n",
      "    </source>\n",
      "\n",
      "    # Logs from systemd-journal for interesting services.\n",
      "    <source>\n",
      "      type systemd\n",
      "      filters [{ \"_SYSTEMD_UNIT\": \"docker.service\" }]\n",
      "      pos_file /var/log/gcp-journald-docker.pos\n",
      "      read_from_head true\n",
      "      tag docker\n",
      "    </source>\n",
      "\n",
      "    <source>\n",
      "      type systemd\n",
      "      filters [{ \"_SYSTEMD_UNIT\": \"kubelet.service\" }]\n",
      "      pos_file /var/log/gcp-journald-kubelet.pos\n",
      "      read_from_head true\n",
      "      tag kubelet\n",
      "    </source>\n",
      "  monitoring.conf: |-\n",
      "    # Prometheus monitoring\n",
      "    <source>\n",
      "      @type prometheus\n",
      "      port 80\n",
      "    </source>\n",
      "\n",
      "    <source>\n",
      "      @type prometheus_monitor\n",
      "    </source>\n",
      "  output.conf: |-\n",
      "    # We use 2 output stanzas - one to handle the container logs and one to handle\n",
      "    # the node daemon logs, the latter of which explicitly sends its logs to the\n",
      "    # compute.googleapis.com service rather than container.googleapis.com to keep\n",
      "    # them separate since most users don't care about the node logs.\n",
      "    <match kubernetes.**>\n",
      "      @type copy\n",
      "\n",
      "      <store>\n",
      "        @type google_cloud\n",
      "\n",
      "        # Set the buffer type to file to improve the reliability and reduce the memory consumption\n",
      "        buffer_type file\n",
      "        buffer_path /var/log/fluentd-buffers/kubernetes.containers.buffer\n",
      "        # Set queue_full action to block because we want to pause gracefully\n",
      "        # in case of the off-the-limits load instead of throwing an exception\n",
      "        buffer_queue_full_action block\n",
      "        # Set the chunk limit conservatively to avoid exceeding the GCL limit\n",
      "        # of 10MiB per write request.\n",
      "        buffer_chunk_limit 2M\n",
      "        # Cap the combined memory usage of this buffer and the one below to\n",
      "        # 2MiB/chunk * (6 + 2) chunks = 16 MiB\n",
      "        buffer_queue_limit 6\n",
      "        # Never wait more than 5 seconds before flushing logs in the non-error case.\n",
      "        flush_interval 5s\n",
      "        # Never wait longer than 30 seconds between retries.\n",
      "        max_retry_wait 30\n",
      "        # Disable the limit on the number of retries (retry forever).\n",
      "        disable_retry_limit\n",
      "        # Use multiple threads for processing.\n",
      "        num_threads 2\n",
      "      </store>\n",
      "      <store>\n",
      "        @type prometheus\n",
      "\n",
      "        <metric>\n",
      "          type counter\n",
      "          name logging_entry_count\n",
      "          desc Total number of log entries generated by either an application container or a system component\n",
      "          <labels>\n",
      "            tag ${tag}\n",
      "            component container\n",
      "          </labels>\n",
      "        </metric>\n",
      "      </store>\n",
      "    </match>\n",
      "\n",
      "    # Keep a smaller buffer here since these logs are less important than the user's\n",
      "    # container logs.\n",
      "    <match **>\n",
      "      @type copy\n",
      "\n",
      "      <store>\n",
      "        @type google_cloud\n",
      "\n",
      "        detect_subservice false\n",
      "        buffer_type file\n",
      "        buffer_path /var/log/fluentd-buffers/kubernetes.system.buffer\n",
      "        buffer_queue_full_action block\n",
      "        buffer_chunk_limit 2M\n",
      "        buffer_queue_limit 2\n",
      "        flush_interval 5s\n",
      "        max_retry_wait 30\n",
      "        disable_retry_limit\n",
      "        num_threads 2\n",
      "      </store>\n",
      "      <store>\n",
      "        @type prometheus\n",
      "\n",
      "        <metric>\n",
      "          type counter\n",
      "          name logging_entry_count\n",
      "          desc Total number of log entries generated by either an application container or a system component\n",
      "          <labels>\n",
      "            tag ${tag}\n",
      "            component system\n",
      "          </labels>\n",
      "        </metric>\n",
      "      </store>\n",
      "    </match>\n",
      "metadata:\n",
      "  name: fluentd-gcp-config\n",
      "  labels:\n",
      "    addonmanager.kubernetes.io/mode: Reconcile\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-53-64453d348dec>\", line 8, in <cell line: 4>\n",
      "    k8s_dataset.append({\"prompt\": get_instruction(content), \"completion\": f\"```yaml\\n{content}\\n```\"})\n",
      "  File \"<ipython-input-42-ee1528a603c0>\", line 2, in get_instruction\n",
      "    chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo-0613\",\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4400 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'examples: 283 dataset:282'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "k8s_dataset = []\n",
    "for path in example_manifests:\n",
    "  with path.open(mode=\"r\", encoding=\"utf-8\") as manifest:\n",
    "    content = manifest.read()\n",
    "    try:\n",
    "      k8s_dataset.append({\"prompt\": get_instruction(content), \"completion\": f\"```yaml\\n{content}\\n```\"})\n",
    "    except:\n",
    "      logging.exception(f\"Exception occured while calling OpenAI. Skipping content: {content}\")\n",
    "\n",
    "\n",
    "f\"examples: {len(example_manifests)} dataset:{len(k8s_dataset)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Udc1BLtQby6Y",
    "outputId": "a8773b1f-3070-40b9-fca9-219d26a368b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268K -rw-r--r-- 1 root root 267K Jun 17 03:39 k8s-instructions.jsonl\n",
      "282 k8s-instructions.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('k8s-instructions.jsonl', 'w') as outfile:\n",
    "    for entry in k8s_dataset:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "! ls -lash k8s-instructions.jsonl\n",
    "! wc -l k8s-instructions.jsonl"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
